{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQY2LAR8s5z"
      },
      "source": [
        "# **Discriminante de Fisher**\n",
        "Luis Felipe Castro Corrales &ensp; 181417  \n",
        "David Emmanuel González Cázares &ensp; 198582  \n",
        "René Ochoa Sawaya &ensp; 179080  \n",
        "<hr>\n",
        "\n",
        "> Describir el modelo y la intuición. Explicar cómo se estiman los parámetros vía máxima verosimilitud. Ilustrar mediante un ejemplo.\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn74Xi3c837N"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "El discriminante de Fisher se utiliza para separar en clases una población (como especies de plantas, marcas de coches o tipos de tumores). Se le asigna una etiqueta con su clase a cada observación. \n",
        "\n",
        "Para discriminar se utilizan las observaciones etiquetadas para construir un clasificador que separe las observaciones en sus clases de la mejor manera posible.\n",
        "\n",
        "Despues se utiliza el clasificador para predecir la clase de observaciones no etiquetadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SasN3S3HG32F"
      },
      "source": [
        "# Clases y atributos\n",
        "\n",
        "La población está partida en clases sin orden. Cada elemento de la población pertenece a una y solo una clase. Todos los elementos tienen medidas variables por los cuales se le clasifica, a estas se les llaman atributos. \n",
        "\n",
        "Ejemplos de atributos pueden ser: de una persona en una población la altura, el peso, la edad; de un foco la luminosidad, su tiempo de vida, su consumo de watts por hora; de una flor su color, su consumo de agua diario su número de pétalos, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXtrlRHmHnym"
      },
      "source": [
        "# Análisis por Discriminante Lineal Gaussiano\n",
        "\n",
        "El Análisis por Discriminación Lineal tiene su origen en una publicación de R. A. Fisher en los *Anales de la Eugenesia* (Fisher,&nbsp;1936). En escencia, el discriminante propuesto por Fisher busca encontrar un vector $\\mathbf{b}$ tal que al realizar la operación $\\mathbf{b}^\\mathrm{T} \\mathbf{x}$ se maximice la separación entre clases. Actualmente, el Análisis por Discriminante Lineal toma supuestos adicionales sobre la distribución de los datos para derivar resultados sobre la probabilidad de error de clasificación y para encontrar los estimadores de máxima verosimilitud de los parámetros desconocidos.\n",
        "\n",
        "Para hacer Análisis por Discriminante Lineal Gausiano, supongamos primero que tenemos dos clases $\\Pi_1$ y $\\Pi_2$ que dan origen a un conjunto de datos. Suponemos que una observación cualquiera $\\mathbf{X}=\\mathbf{x}$ proviene de la clase $\\Pi_i$ con una *probabilidad previa* $\\pi_i$. Además supongamos que la densidad de probabilidad condicional de $\\mathbf{x}$ para la clase $i$ es \n",
        "$$\n",
        "\\mathrm{P}(\\mathbf{X}=\\mathbf{x}|\\mathbf{X}\\in\\Pi_i) = f_i(\\mathbf{x}), \\quad i =1,2.\n",
        "$$\n",
        "Recordemos que la regla de clasificación de Bayes asigna $\\mathbf{x}$ a la clase $\\Pi_i$ con la mayor *probabilidad posterior* $p(\\Pi_i|\\mathbf{x}) = \\mathrm{P}(\\mathbf{X}\\in\\Pi_i|\\mathbf{X}=\\mathbf{x})$. Así, por la Regla de Bayes, determinamos que se asigna $\\mathbf{X}$ a $\\Pi_1$ si\n",
        "\\begin{equation}\n",
        "\\tag{1} \\frac{f_1(\\mathbf{x})\\pi_1}{f_2(\\mathbf{x})\\pi_2} > 1\n",
        "\\end{equation}\n",
        "\n",
        "El Análisis por Discriminante Lineal Gausiano parte del la regla de clasificación de Bayes (1) pero añade el supuesto de que ambas densidades de probabilidad son distribuciones normales multivariadas con medias arbitrarias pero con una matriz de covarianza en común. De tal forma al sustituir las funciones $f_i(\\cdot)$ y aplicando logaritmo natural, obtenemos\n",
        "\\begin{equation}\n",
        "\\tag{2} L(\\mathbf{x}) = \\ln\\left\\{\\frac{f_1(\\mathbf{x})\\pi_1}{f_2(\\mathbf{x})\\pi_2}\\right\\} = b_0 + \\mathbf{b}^\\mathrm{T} \\mathbf{x}\n",
        "\\end{equation}\n",
        "que es una función lineal de $\\mathbf{x}$ donde\n",
        "\\begin{equation}\n",
        "\\tag{3} \\mathbf{b} = \\Sigma_\\mathbf{XX}^{-1}(\\mu_1-\\mu_2)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\tag{4} b_0 = -\\frac{1}{2}\\left\\{\\mu_1^\\mathrm{T}\\Sigma_\\mathbf{XX}^{-1}\\mu_1 - \\mu_2^\\mathrm{T}\\Sigma_\\mathbf{XX}^{-1}\\mu_2\\right\\} + \\ln\\left\\{\\frac{\\pi_1}{\\pi_2}\\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "Así, se asigna $\\mathbf{X}$ a $\\Pi_1$ si $L(\\mathbf{x})>0$, o de lo contrario, se asigna $\\mathbf{X}$ a $\\Pi_2$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heRjTgPNIcbo"
      },
      "source": [
        "## Probabilidad de error de clasificación\n",
        "\n",
        "Las observaciones se clasifican según en cual de las dos regiones de clasificación caegan. La probabilidad de clasificar erroneamente la observación es la probabilidad de que caega en la región de la clase con la que no está etiquetada. La probabilidad de error de clasificación incrementa conforme la observación se vuelve más equidistante entre las regiones de claificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BbuIvagIuWl"
      },
      "source": [
        "## Estimadores muestrales\n",
        "\n",
        "La estimación de medias de cada clase se estiman mediante las medias muestrales de los datos observados en cada clase $hat{\\boldsymbol{\\mu}}j = \\frac{1}{n_j} \\sum{i=1}^{n_j} \\mathbf{x}{ij}$. Y la estimación de la covarianza común se utiliza la matriz de covarianza muestral $\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N - K} \\sum{j=1}^{K} \\sum_{i=1}^{n_j} (\\mathbf{x}{ij} - \\hat{\\boldsymbol{\\mu}}_j)(\\mathbf{x}{ij} - \\hat{\\boldsymbol{\\mu}}_j)^T$. Estos dos son los estimadores de máxima verosimilitud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkONsjAzI48N"
      },
      "source": [
        "# Implementación y ejemplo de clasificación\n",
        "En esta sección crearemos una función que genere un clasificador lineal y lo probaremos con la base de datos _inserte nombre de base de datos_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class LDAGaussiano:\n",
        "    def __init__(self):\n",
        "        b0 = None\n",
        "        b = None\n",
        "        clases = None\n",
        "\n",
        "\n",
        "    def entrenar(self, datos, clasific):\n",
        "        a,b = datos.shape\n",
        "        self.clases, inv, counts = np.unique(clasific, return_inverse = True, return_counts = True)\n",
        "        mu_1, mu_2 = np.zeros(b), np.zeros(b)\n",
        "        sxx_1, sxx_2 = np.zeros((b,b)), np.zeros((b,b))\n",
        "\n",
        "        for i in range(0,a):\n",
        "            if clasific[inv[i]] == self.clases[0]:\n",
        "                mu_1 += datos[i,::]\n",
        "            else:\n",
        "                mu_2 += datos[i,::]\n",
        "        mu_1 /= counts[0]\n",
        "        mu_2 /= counts[1]\n",
        "\n",
        "        for i in range(0,a):\n",
        "            if clasific[inv[i]] == self.clases[0]:\n",
        "                sxx_1 += np.outer(datos[i,::]-mu_1,datos[i,::]-mu_1)\n",
        "            else:\n",
        "                sxx_2 += np.outer(datos[i,::]-mu_2,datos[i,::]-mu_2)\n",
        "        sigma_xx = (sxx_1 + sxx_2)/a\n",
        "\n",
        "        self.b0 = -(np.inner(mu_1,np.linalg.solve(sigma_xx,mu_1)) - np.inner(mu_2,np.linalg.solve(sigma_xx,mu_2)))/2 + np.log(counts[0]/counts[1])\n",
        "        self.b = np.linalg.solve(sigma_xx,(mu_1-mu_2))\n",
        "\n",
        "\n",
        "    def predecir(self,datos):\n",
        "        if self.b0 and self.b:\n",
        "            a,b = datos.shape\n",
        "            sol = np.zeros(a)\n",
        "            for index, x in enumerate(datos):\n",
        "                L = self.b0 + np.inner(self.b,x)\n",
        "                if L > 0:\n",
        "                    sol[index] = self.clases[0]\n",
        "                else:\n",
        "                    sol[index] = self.clases[1]\n",
        "        else:\n",
        "            print(\"Debes entrenar el modelo.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 10 100\n",
            "1 11 110\n",
            "2 12 120\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Felip\\AppData\\Local\\Temp\\ipykernel_2596\\2845770709.py:7: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  print(index, row[1], row[2])\n"
          ]
        },
        {
          "ename": "LinAlgError",
          "evalue": "Singular matrix",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[1;32md:\\Users\\Felip\\Documents\\ITAM\\Semestre 11\\Aplicada 3\\Proyecto\\Discriminante_de_Fisher.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m clasific \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m lda \u001b[39m=\u001b[39m LDAGaussiano()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m lda\u001b[39m.\u001b[39;49mentrenar(datos,clasific)\n",
            "\u001b[1;32md:\\Users\\Felip\\Documents\\ITAM\\Semestre 11\\Aplicada 3\\Proyecto\\Discriminante_de_Fisher.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m         sxx_2 \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mouter(datos[i,::]\u001b[39m-\u001b[39mmu_2,datos[i,::]\u001b[39m-\u001b[39mmu_2)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m sigma_xx \u001b[39m=\u001b[39m (sxx_1 \u001b[39m+\u001b[39m sxx_2)\u001b[39m/\u001b[39ma\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb0 \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m(np\u001b[39m.\u001b[39minner(mu_1,np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msolve(sigma_xx,mu_1)) \u001b[39m-\u001b[39m np\u001b[39m.\u001b[39minner(mu_2,np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msolve(sigma_xx,mu_2)))\u001b[39m/\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39mlog(counts[\u001b[39m0\u001b[39m]\u001b[39m/\u001b[39mcounts[\u001b[39m1\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39msolve(sigma_xx,(mu_1\u001b[39m-\u001b[39mmu_2))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Users/Felip/Documents/ITAM/Semestre%2011/Aplicada%203/Proyecto/Discriminante_de_Fisher.ipynb#X12sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb0, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mb\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\linalg\\linalg.py:409\u001b[0m, in \u001b[0;36msolve\u001b[1;34m(a, b)\u001b[0m\n\u001b[0;32m    407\u001b[0m signature \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDD->D\u001b[39m\u001b[39m'\u001b[39m \u001b[39mif\u001b[39;00m isComplexType(t) \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mdd->d\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    408\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 409\u001b[0m r \u001b[39m=\u001b[39m gufunc(a, b, signature\u001b[39m=\u001b[39;49msignature, extobj\u001b[39m=\u001b[39;49mextobj)\n\u001b[0;32m    411\u001b[0m \u001b[39mreturn\u001b[39;00m wrap(r\u001b[39m.\u001b[39mastype(result_t, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m\"\u001b[39m\u001b[39mSingular matrix\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})\n",
        "df = df.reset_index()  # make sure indexes pair with number of rows\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    print(index, row[1], row[2])\n",
        "\n",
        "datos = np.array([[1, 3, 3], [4, 5, 6],[1,9,3],[1,2,3]])\n",
        "clasific = np.array([1,2,1,1])\n",
        "lda = LDAGaussiano()\n",
        "lda.entrenar(datos,clasific)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O1Wj5wz8zcY"
      },
      "source": [
        "  # Referencias\n",
        "  \n",
        "  Izenman, A. J. (2008). *Modern multivariate statistical techniques* (Vol. 1). New York: Springer.\n",
        "\n",
        "  Fisher, R. A. (1936). *The use of multiple measurements in taxonomic problems*. Annals of eugenics, 7(2), 179-188."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

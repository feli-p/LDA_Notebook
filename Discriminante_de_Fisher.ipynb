{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPQY2LAR8s5z"
      },
      "source": [
        "# **Discriminante de Fisher**\n",
        "Luis Felipe Castro Corrales &ensp; 181417  \n",
        "David Emmanuel González Cázares &ensp; 198582  \n",
        "René Ochoa Sawaya &ensp; 179080  \n",
        "<hr>\n",
        "\n",
        "Describir el modelo y la intuición. Explicar cómo se estiman los parámetros vía máxima verosimilitud. Ilustrar mediante un ejemplo.\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wn74Xi3c837N"
      },
      "source": [
        "# Introducción\n",
        "\n",
        "El discriminante de Fisher se utiliza para separar en clases una población (como especies de plantas, marcas de coches o tipos de tumores). Se le asigna una etiqueta con su clase a cada observación. \n",
        "\n",
        "Para discriminar se utilizan las observaciones etiquetadas para construir un clasificador que separe las observaciones en sus clases de la mejor manera posible.\n",
        "\n",
        "Despues se utiliza el clasificador para predecir la clase de observaciones no etiquetadas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SasN3S3HG32F"
      },
      "source": [
        "# Clases y atributos\n",
        "\n",
        "La población está partida en clases sin orden. Cada elemento de la población pertenece a una y solo una clase. Todos los elementos tienen medidas variables por los cuales se le clasifica, a estas se les llaman atributos. \n",
        "\n",
        "Ejemplos de atributos pueden ser: de una persona en una población la altura, el peso, la edad; de un foco la luminosidad, su tiempo de vida, su consumo de watts por hora; de una flor su color, su consumo de agua diario su número de pétalos, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXtrlRHmHnym"
      },
      "source": [
        "# Análisis por Discriminante Lineal Gaussiano\n",
        "\n",
        "El Análisis por Discriminación Lineal tiene su origen en una publicación de R. A. Fisher en los *Anales de la Eugenesia* (Fisher,&nbsp;1936). En escencia, el discriminante propuesto por Fisher busca encontrar un vector $\\mathbf{b}$ tal que al realizar la operación $\\mathbf{b}^\\mathrm{T} \\mathbf{x}$ se maximice la separación entre clases. La función discriminante propuesta Fisher difiere a la versión actual del Análisis por Discriminante Lineal en que no se suponía nada sobre la distribución de los datos.\n",
        "\n",
        "Sean $\\Pi_1$ y $\\Pi_2$ dos clases que dan origen a una serie de datos. Suponemos que una observación cualquiera $\\mathbf{X}=\\mathbf{x}$ proviene de la clase $\\Pi_i$ con una *probabilidad previa* $\\pi_i$. Además supongamos que la densidad de probabilidad condicional de $\\mathbf{x}$ para la clase $i$ es \n",
        "$$\n",
        "\\mathrm{P}(\\mathbf{X}=\\mathbf{x}|\\mathbf{X}\\in\\Pi_i) = f_i(\\mathbf{x}), \\quad i =1,2.\n",
        "$$\n",
        "Recordemos que la regla de clasificación de Bayes asigna $\\mathbf{x}$ a la clase $\\Pi_i$ con la mayor *probabilidad posterior* $p(\\Pi_i|\\mathbf{x}) = \\mathrm{P}(\\mathbf{X}\\in\\Pi_i|\\mathbf{X}=\\mathbf{x})$. Así, por la Regla de Bayes, determinamos que se asigna $\\mathbf{X}$ a $\\Pi_1$ si\n",
        "<a name=\"eq:bayes\"></a>\n",
        "\\begin{equation}\n",
        "\\tag{1} \\frac{f_1(\\mathbf{x})\\pi_1}{f_2(\\mathbf{x})\\pi_2} > 1\n",
        "\\end{equation}\n",
        "\n",
        "El Análisis por Discriminante Lineal Gausiano parte del la regla de clasificación de Bayes [(1)](#eq:bayes) pero añade el supuesto de que ambas densidades de probabilidad son distribuciones normales multivariadas con medias arbitrarias pero con una matriz de covarianza en común. De tal forma al sustituir las funciones $f_i(\\cdot)$ y aplicando logaritmo natural, obtenemos\n",
        "\\begin{equation}\n",
        "\\tag{2} L(\\mathbf{x}) = \\ln\\left\\{\\frac{f_1(\\mathbf{x})\\pi_1}{f_2(\\mathbf{x})\\pi_2}\\right\\} = b_0 + \\mathbf{b}^\\mathrm{T} \\mathbf{x}\n",
        "\\end{equation}\n",
        "que es una función lineal de $\\mathbf{x}$ donde\n",
        "\\begin{equation}\n",
        "\\tag{3} \\mathbf{b} = \\Sigma_\\mathbf{XX}^{-1}(\\mu_1-\\mu_2)\n",
        "\\end{equation}\n",
        "\n",
        "\\begin{equation}\n",
        "\\tag{4} b_0 = -\\frac{1}{2}\\left\\{\\mu_1^\\mathrm{T}\\Sigma_\\mathbf{XX}^{-1}\\mu_1 - \\mu_2^\\mathrm{T}\\Sigma_\\mathbf{XX}^{-1}\\mu_2\\right\\} + \\ln\\left\\{\\frac{\\pi_1}{\\pi_2}\\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "Así, se asigna $\\mathbf{X}$ a $\\Pi_1$ si $L(\\mathbf{x})>0$ o, de lo contrario, se asigna $\\mathbf{X}$ a $\\Pi_2$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heRjTgPNIcbo"
      },
      "source": [
        "## Probabilidad de error de clasificación\n",
        "\n",
        "Las observaciones se clasifican según en cual de las dos regiones de clasificación caegan. La probabilidad de clasificar erroneamente la observación es la probabilidad de que caega en la región de la clase con la que no está etiquetada. La probabilidad de error de clasificación incrementa conforme la observación se vuelve más equidistante entre las regiones de claificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BbuIvagIuWl"
      },
      "source": [
        "## Estimadores muestrales\n",
        "\n",
        "La estimación de medias de cada clase se estiman mediante las medias muestrales de los datos observados en cada clase $hat{\\boldsymbol{\\mu}}j = \\frac{1}{n_j} \\sum{i=1}^{n_j} \\mathbf{x}{ij}$. Y la estimación de la covarianza común se utiliza la matriz de covarianza muestral $\\hat{\\boldsymbol{\\Sigma}} = \\frac{1}{N - K} \\sum{j=1}^{K} \\sum_{i=1}^{n_j} (\\mathbf{x}{ij} - \\hat{\\boldsymbol{\\mu}}_j)(\\mathbf{x}{ij} - \\hat{\\boldsymbol{\\mu}}_j)^T$. Estos dos son los estimadores de máxima verosimilitud."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkONsjAzI48N"
      },
      "source": [
        "# Implementación y ejemplo de clasificación\n",
        "En esta sección crearemos una función que genere un clasificador lineal y lo probaremos con la base de datos _inserte nombre de base de datos_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4O1Wj5wz8zcY"
      },
      "source": [
        "  # Referencias\n",
        "  \n",
        "  Izenman, A. J. (2008). *Modern multivariate statistical techniques* (Vol. 1). New York: Springer.\n",
        "\n",
        "  Fisher, R. A. (1936). *The use of multiple measurements in taxonomic problems*. Annals of eugenics, 7(2), 179-188."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
